# Curious Replay：リプレイバッファからのサンプリング（実装向け）

本論文の提案は「Prioritized Experience Replay (PER) の枠組み」を使いつつ、
priority（優先度）を TD 誤差ではなく **“curiosity（不慣れさ/苦手さ）”** で定義して
サンプリング分布を作る、というもの。

---

## 1. 何を「experience」として優先度付けするか

論文中では experience = **単一の遷移**（1-step transition）
- `i` 番目の遷移: `(x_t, a_t, r_t, x_{t+1})`
- visit count `v_i`: その遷移が **学習バッチに入った回数**（revisited 回数）

※ Dreamer 系は実装上「シーケンス（連続した長さ H の区間）」をサンプルすることが多い。
その場合でも、以下いずれかで整合を取れる：
1) **遷移単位の priority を持ち**、サンプルは「開始インデックス i」を引く（i から H 区間を取り出す）
2) シーケンス単位にまとめ、priority をシーケンス内遷移の `mean/max` などで定義する  
   （論文の厳密さは 1) に近いが、実装都合で 2) も現実的）

---

## 2. バッファに保持するメタデータ（最小構成）

固定容量 `N` のリプレイバッファをリングバッファとして持つとする。

各スロット（遷移 i）に対して：
- `data[i]`: 遷移データ（観測/行動/報酬/次観測 + 必要なら done 等）
- `p[i]`: priority（>0 を保証）
- `v[i]`: visit count（int, 初期 0）
- （任意）`age[i]` や `timestamp`：不要（優先度式に含まれない）

さらに、**SumTree**（セグメント木）を持つ：
- 葉に `p[i]` を保持し、根に `sum(p)` を保持
- `sample` と `update` を O(log N) で実行するため

---

## 3. 優先度の定義（Count-based / Adversarial / Combined）

### 3.1 Count-based priority（「まだ十分リプレイしていない」を優先）
- ハイパーパラメータ：`β ∈ [0,1]`
- 定義：
  - `p_count(i) = β^(v_i)`

性質：
- `v_i=0` の新データは `1`
- バッチに入るたび `v_i` が増えて指数的に減衰 → “まんべんなく再学習” を促す

### 3.2 Adversarial priority（「世界モデルが苦手」を優先）
- ハイパーパラメータ：`α ∈ [0,1]`, `ε > 0`
- 各遷移 i に対して、その遷移での世界モデル損失 `L_i` を使い：
  - `p_adv(i) = (|L_i| + ε)^α`

注意（論文の実装方針）：
- `L_i` は「その遷移が学習に使われた時」に計算し、
  **priority 更新もその時だけ**行う（未サンプル遷移の priority は古いままでもOKという扱い）

### 3.3 Curious Replay priority（Combined）
- ハイパーパラメータ：`c > 0`（スケール係数）
- 定義（論文の式(1)）：
  - `p(i) = c * β^(v_i) + (|L_i| + ε)^α`

直感：
- **新しい（or あまり見てない）** → `c * β^(v_i)` が効く
- **苦手（loss が大きい）** → `( |L_i|+ε )^α` が効く
- 足し算で両方を同時に満たす方向へ寄せる

---

## 4. サンプリング分布（Replay buffer からどう引くか）

リプレイバッファ内の全遷移（有効なスロット）について、
- `P(i) = p(i) / Σ_j p(j)`

これが「サンプリング確率」。

SumTree を使うと：
- `total = Σ_j p(j)` を根が持つ
- `u ~ Uniform(0, total)` を引いて、累積和の位置で葉（= index i）を探索できる

---

## 5. アルゴリズム（学習ループに組み込む手順）

### 5.1 主要ハイパーパラメータ
- `L_env_per_train`: 1回の学習イテレーションあたり環境から収集する遷移数（論文 Algorithm 1 の L）
- `B`: バッチサイズ（サンプル遷移数）
- `p_MAX`: 新規追加遷移の初期 priority（大きめ）
- `c, β, α, ε`: 上記の式の係数

### 5.2 1イテレーションの処理フロー（論文 Algorithm 1 準拠）
1) 環境から `L_env_per_train` 個の遷移を収集
2) 各遷移をリプレイバッファへ追加  
   - `p[i] ← p_MAX`
   - `v[i] ← 0`
   - SumTree の該当 leaf を `p_MAX` に更新
3) SumTree から `B` 個サンプル（確率 P(i) に従う）
4) サンプルバッチで世界モデル＆方策を学習し、各遷移 i の損失 `L_i` を得る（キャッシュ）
5) バッチ内の各遷移 i について priority 更新：
   - `p[i] ← c * β^(v[i]) + (|L_i| + ε)^α`
   - `v[i] ← v[i] + 1`
   - SumTree の leaf を新しい `p[i]` に更新

---

## 6. SumTree（実装の要点）

### 6.1 データ構造（典型）
- `tree`: 長さ `2*N` の配列（1-indexed でも 0-indexed でもOK）
  - 葉：`tree[base + i] = p[i]`
  - 親：`tree[k] = tree[2k] + tree[2k+1]`

### 6.2 update(i, new_p)
- `delta = new_p - old_p`
- 葉から根へ向かって `tree[node] += delta` を伝播

### 6.3 sample(u)
- `node = root`
- `while node is not leaf`:
  - `if u <= left_sum: node = left`
  - `else: u -= left_sum; node = right`
- leaf に到達したら、その leaf が指す `i` を返す

---

## 7. Loss L_i をどう作るか（Dreamer 系への落とし込み）

論文では世界モデル損失として
- `L = L_image + L_reward + L_KL`
を用いる（Dreamer の world model 学習損失）。  
実装上のポイントは「サンプル単位 i に L_i を割り当てられること」。

### 7.1 遷移単位で学習している場合
- そのまま `L_i` を計算して優先度更新に使う

### 7.2 シーケンス（長さ H）で学習している場合（よくある）
以下のどれかを選べる（どれも実装しやすい）：
- (A) **時刻ごとの loss（per-step loss）** が取れるなら、それを各遷移 i に対応付け
- (B) シーケンス損失 `L_seq` を、そのシーケンス内の全遷移に同じ値として割当（簡単）
- (C) シーケンス開始点 i の priority として `L_seq` を使う（開始点サンプリング型）

論文の “transition i ごとに loss をキャッシュして更新” に最も近いのは (A)。

---

## 8. 実装上の注意・落とし穴

- **priority は必ず正**：`ε > 0` を入れて 0 を避ける（特に α=0 のときも安定）
- **新規データを強く優先**：追加時に `p_MAX` を与える（新データが早く学習に入る）
- **priority の更新頻度**：未サンプル遷移は古い priority のまま（論文の簡略化方針）
- **c のスケーリング**：
  - `β^(v)` は最大 1 なので、`c` が小さいと count 項が効きにくい
  - loss 項のスケールも環境・損失設計で変わるので、`c` は要調整
- **with/without replacement**：
  - 論文は明記していない。実装は with replacement（独立に B 回 sample）が楽
  - 重複が嫌なら再サンプルで回避（ただし理論分布から少しズレる）

---

## 9. 参考：論文に記載の一例ハイパーパラメータ
- `β = 0.7`, `α = 0.7`, `c = 1e4`, `ε = 0.01`, `p_MAX = 1e5`
（論文中の実験設定例）
